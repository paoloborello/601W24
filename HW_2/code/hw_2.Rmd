---
title: "HW 2"
author: "Paolo Borello"
date: "2024-01-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
library(ggplot2)
library(plotly)

library(Matrix)
library(EFA.dimensions)

library(tidyverse)
```

```{r}
# My UMID
set.seed(65987833)
```


## Q1

Principle Components Regression would perform better than lasso and ridge whenever our covariates shows high levels of multi-collinearity.\\
Note that in this setting PCA helps us create independent covariates and perform 'hard'-shrinkage. In the case of Ridge and LASSO on the other hand we shrink covariates' coefficients directly.

In practice high correlation may happen in biological applications with genomic data, since groups of genes can be high correlated with each other. In this case usually we also deal with high-dimensional data therefore dimensionality reduction is needed and PCR should perform better than Ridge or LASSO.

Here I perform a simulation study with 200 simulations where multi-collinear data is generated by first creating 2 covariates and then the remaining ones are randomly-weighted convex combinations of these 2. I fix my coefficients to be all ones, before generating my response. I evaluate the three models based on MSE and report a boxplot of my findings.

```{r}
sim_data <- function(n = 1000, q = 100){
  X <- matrix(0, nrow = n, ncol = q)
  # sample random normals for the first two columns
  X[ , 1] <- rnorm(n)
  X[ , 2] <- rnorm(n)
  
  # the other columns are random convex combinations of the first two columns
  for(i in 3:q){
    weight <- runif(1)
    X[ , i] <- weight * X[ , 1] + (1 - weight) * X[ , 2]
  }
  
  # generate beta and response
  beta <- rep(1,q)
  eps <- rnorm(n)
  y <- X %*% beta + eps
  
  # train/test split
  train_index <- sample(1:n, size = n/2)
  test_index <- setdiff(1:n, train_index)

  X_train <- X[train_index, ]
  y_train <- y[train_index]
  X_test <- X[test_index, ]
  y_test <- y[test_index]
  
  return(list(
    X_train = scale(X_train),
    y_train = scale(y_train),
    X_test = scale(X_test),
    y_test = scale(y_test)
  ))
}
```

```{r sim1}
nsim <- 200

mse_list = list(
  PCR = rep(0, nsim),
  Lasso = rep(0, nsim),
  Ridge = rep(0, nsim),
  Lasso_indicator = rep(0, nsim),
  ridge_indicator = rep(0, nsim)
)

for(i in 1:nsim){
  data <- sim_data()
  
  ## PCA Regression
  # get PCA
  pca <- prcomp(data$X_train, center = FALSE, rank. = 2)
  
  # regress Y on PC components
  df <- as.data.frame(pca$x)
  df <- cbind(data$y_train, df) |>
    rename(y_train = "data$y_train")
  
  lm_model <- lm(y_train ~ . + 0, data = df)
  # predict Y
  y_predict <- data$X_test %*% pca$rotation %*% matrix(lm_model$coefficients)
  
  
  ## LASSO and Ridge
  lasso <- glmnet::cv.glmnet(x = data$X_train, y = data$y_train, 
                             intercept = F, alpha = 1)
  y_pred_lasso <- predict(lasso, newx = data$X_test, 
                          s = "lambda.min")
  
  ridge <- glmnet::cv.glmnet(x = data$X_train, y = data$y_train, 
                                   intercept = F, alpha = 0)
  y_predict_ridge <- predict(ridge, newx = data$X_test, s = "lambda.min")
  
  mse_list$PCR[i] <- mean((data$y_test - y_predict)^2)
  mse_list$Lasso[i] <- mean((data$y_test - y_pred_lasso)^2)
  mse_list$Lasso_indicator[i] <- mse_list$PCR[i] < mse_list$Lasso[i]
  mse_list$Ridge[i] <- mean((data$y_test - y_predict_ridge)^2)
  mse_list$Ridge_indicator[i] <- mse_list$PCR[i] < mse_list$Ridge[i]
}
```


```{r}
# mean MSE
mse_list$PCR |> mean()
mse_list$Lasso |> mean()
mse_list$Ridge |> mean()

## Proportion of Time PCA Regression performs better than LASSO and Ridge
sum(mse_list$Lasso_indicator)/nsim
sum(mse_list$Ridge_indicator)/nsim
```

```{r, warning = FALSE, echo=F}
mse_list |>
  as.data.frame() |>
  select(PCR, Lasso, Ridge) |>
  pivot_longer(
    cols = 1:3,
    names_to = "type",
    values_to = "value"
  ) |>
  ggplot(aes(x = type, y = value, color = type)) +
  geom_boxplot() +
  theme_bw() +
  labs(
    y = "Test Data MSE",
    title = "Test Data Prediction MSE of Various Regressions",
    color = "Regression Type"
  ) + 
  scale_y_continuous(limits = c(0, 0.005))
```

Therefore we can see that PCR consistently performs better than LASSO and Ridge in our simulation study.

## Q3

### (a)

The code to retrieve PC direction is the following
```{r}
load("nytimes.RData")

Y <- nyt.frame[ , -1]
labels <- nyt.frame[ , 1]
mu <- colMeans(Y)
Y <- scale(Y, scale = FALSE)

pc_dir <- svd(Y)$v
```

### (b)
```{r}
X_proj1 <- Y %*% pc_dir[ , 1]
X_proj2 <- Y %*% pc_dir[ , 1:2]
X_proj3 <- Y %*% pc_dir[ , 1:3]

dat_1d = cbind(factor(labels), data.frame(X_proj1))
dat_2d = cbind(factor(labels), data.frame(X_proj2))
dat_3d = cbind(factor(labels), data.frame(X_proj3))
```

```{r, echo=F}
ggplot(data = dat_1d, aes(x = X_proj1, y = 0, color = labels)) + 
  geom_point(size=2) +
  labs(x = 'PC1', y = '', title = '1D Projection') +
  theme_bw()
```
```{r, echo=F}
ggplot(data = dat_2d, aes(x=X1, y=X2, color=labels)) + 
  geom_point(size=2) +
  labs(x='PC1', y='PC2', title = '2D Projection') + 
  theme_bw()
```
```{r, echo=F}
plot_ly(dat_3d, x = ~X1, y = ~X2, z = ~X3, color = ~labels, 
        type = 'scatter3d', mode = 'markers',
        marker = list(size = 10)) |>
  layout(scene = list(xaxis = list(title = 'PC 1'),
                      yaxis = list(title = 'PC 2'),
                      zaxis = list(title = 'PC 3')))
```

We achieve a good level of separation in our data with 2 dimensions. 
Including the third PC does not help us in creating more/better separation.

### (c)

```{r}
nwords <- 20
top_words_mat <- matrix("",nrow=nwords, ncol=3)
bot_words_mat <- matrix("",nrow=nwords, ncol=3)
for (i in 1:3) {
  top_words <- colnames(Y)[sort(pc_dir[,i], decreasing=T, index.return=T)$ix[1:nwords]]
  bottom_words <- colnames(Y)[sort(pc_dir[,i], decreasing=F, index.return=T)$ix[1:nwords]]
  top_words_mat[,i] <- top_words
  bot_words_mat[,i] <- bottom_words
}

top_words_mat
bot_words_mat
```

Therefore we can see that PC1 is positively correlated with words involving music and negatively with art.
PC2 does the opposite and positive loadings correspond to art, while negative ones to music.
PC3 replicates the behavior of PC1.

## Q4

### Data Generation
```{r}
Lambda <- rbind(cbind(rep(1, 10), rep(0, 10)), cbind(rep(0, 10), rep(1, 10)))
mu <- rep(0, 20)

nsamples = 100

gen_sample <- function(nsamples=100, Lambda, mu){
  X <- matrix(rnorm(2 * nsamples, 0, 1), ncol=2)
  W <- matrix(rnorm(nsamples*20, 0, sqrt(0.5)), nrow=nsamples)
  Y <- as.matrix(X %*% t(Lambda) + W)
  return(Y)
}

Y_original = gen_sample(nsamples=100, Lambda, mu)
```

### (a)

In the .pdf

### (b)

Equation of subspace is in .pdf

```{r}
X_FA <- factanal(Y_original, factors = 2, scores = 'regression')$scores
```
```{r, echo=F}
# Plot projections onto 2D subspace
ggplot() +
  geom_point(mapping = aes(x = X_FA[, 1], y = X_FA[, 2]), color='orange', shape=17) + 
  geom_vline(xintercept=0, linetype=2, size=0.3) + 
  geom_hline(yintercept=0, linetype=2, size=0.3) + 
  labs(title="Projection in 2D Subspace", 
       x = "Factor 1", y = "Factor 2")
```

### (c)
```{r}
PCA <- prcomp(Y_original)
X_PCA <- PCA$x

n = dim(X_PCA)[1]
df <- data.frame(
  X1 = c(X_PCA[, 1], X_FA[, 1]),
  X2 = c(X_PCA[, 2], X_FA[, 2]),
  Method = rep(c("PCA", "FA"), each = n)
)
```
```{r, echo=F}
ggplot(df, aes(x = X1, y = X2, color = Method, shape = Method)) +
  geom_point(alpha=0.65) +
  labs(title = "Scatterplot of PCA and FA Projections",
       x = "PC/Factor 1", y = "PC/Factor 2") +
  scale_color_manual(values = c("orange", "red")) + 
  scale_shape_manual(values = c(17, 16)) + 
  geom_vline(xintercept=0, linetype=2, size=0.3) + 
  geom_hline(yintercept=0, linetype=2, size=0.3)
```

As we can see PCA projections tend to have more variance. This is expected since in PCA we capture more variance by choosing the top eigenvectors since we want to maximize explained variance of our data.

## Q5

### (a)
```{r}
# function to permute elements inside each column of a matrix
permute_obs <- function(Y){
  n = dim(Y)[1]
  p = dim(Y)[2]
  for (i in 1:p) {
    Y[,i] <- sample(Y[,i], n)
  }
  return(Y)
}

# function to retrieve eigenvalues of sample Y covariance matrix
eigenvalues <- function(Y){
  S = cov(Y)
  return(eigen(S)$values)
}

# function to retrieve the empirical permuted distribution of eigenvalues
empirical_distr <- function(Y, nrep=500){
  emp <- matrix(0, nrow=nrep, ncol=20)
  for (i in 1:nrep) {
    emp[i,] <- eigenvalues(permute_obs(Y))
  }
  return(emp)
}

# function to retrieve upper quantiles given an empirical distribution matrix
emp_quantiles <- function(emp, level=0.95){
  return(apply(emp, MARGIN = 2, quantile, level))
}

# function to choose the number of PCs using the permutation distributions
perm_dec_rule <- function(Y, nrep=500, level=0.95){
  hat_eigen <- eigenvalues(Y)
  boolean <- (hat_eigen > emp_quantiles(empirical_distr(Y, nrep=nrep), 
                                        level=level))
  return(match(FALSE, boolean)-1)
}

# number of PCs selected
perm_dec_rule(Y_original)
```

### (b)
```{r data, cache=TRUE}
# for each simulation we create a new sample and get the number of selected PCs
nsim = 100
ns_pc = rep(0, nsim)
for (i in 1:nsim) {
  Y <- gen_sample(nsamples=100, Lambda, mu)
  ns_pc[i] <- perm_dec_rule(Y)
}
```

```{r}
# how many times the simulation estimates the correct number of PCs
sum(ns_pc == 2)
```

### (c)
```{r}
# function to retrieve the number of components based on explained variance
var_dec_rule <- function(Y, prop=0.9){
  eig_vec <- eigenvalues(Y)
  # proportion of explained variance is retrieved using 
  # cumsum on eigenvalue vector
  boolean <- (cumsum(eig_vec / sum(eig_vec)) < prop)
  return(match(FALSE, boolean) + 1)
}

var_dec_rule(Y_original)

# for each simulation we create a new sample and get the number of selected PCs
nsim = 100
ns_pc_2 = rep(0, nsim)
for (i in 1:nsim) {
  Y <- gen_sample(nsamples=100, Lambda, mu)
  ns_pc_2[i] <- var_dec_rule(Y)
}
# how many times the simulation estimates the correct number of PCs
sum(ns_pc_2 == 2)
```

### (d)
```{r}
# number of factors chosen using the likelihood ratio test
SMT(Y_original, verbose = F)$NfactorsSMT
```

### (e)
```{r}
# for each simulation we create a new sample and get the number of selected PCs
library(EFA.dimensions)
nsim = 100
ns_pc_3 = rep(0, nsim)
for (i in 1:nsim) {
  Y <- gen_sample(nsamples=100, Lambda, mu)
  ns_pc_3[i] <- SMT(Y, verbose = F)$NfactorsSMT
}
# how many times the simulation estimates the correct number of PCs
sum(ns_pc_3 == 2)
```


