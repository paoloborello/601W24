\documentclass[10pt,notitlepage]{article}
%Mise en page
\usepackage[left=1.5cm, right=1.5cm, lines=45, top=0.8in, bottom=0.7in]{geometry}
\usepackage{fancyhdr}
\usepackage{float}
\pagestyle{fancy}
\usepackage[most,breakable]{tcolorbox}
\usepackage{pdfcol,xcolor}
\usepackage{tikz}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{graphicx} %Loading the package
\graphicspath{{./images/}}
\usepackage{dsfont}
\usepackage{amssymb,amsmath,mathtools}
\usepackage{xspace}
\usepackage[normalem]{ulem}
\usepackage{bm}
\usepackage[breaklinks=true,colorlinks,linkcolor=magenta,urlcolor=magenta,citecolor=black]{hyperref}
\usepackage{cleveref}
\usepackage{xpatch}
\usepackage[shortlabels]{enumitem}
\xpretocmd{\algorithm}{\hsize=\linewidth}{}{}
\definecolor{MBlue}{RGB}{0,39,76}
\allowdisplaybreaks

\newtcolorbox[auto counter]{exercise}[1][]{%
	colback=yellow!10,
	colframe=MBlue,
	coltitle=white,
	use color stack,
	enforce breakable,
	enhanced,
	fonttitle=\bfseries,
	before upper={\parindent15pt\noindent}, 
	title={\color{white} #1}
}

\lhead{
	\textbf{University of Michigan}
}
\rhead{
	\textbf{Winter 24}
}
\chead{
	\textbf{STATS 601}
}
\lfoot{}
\cfoot{Paolo Borello, borello@umich.edu}

\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\MBlue}[1]{{\color{MBlue}#1}}
\newcommand{\blue}[1]{{\color{blue}#1}}
\newcommand{\magenta}[1]{{\color{magenta}#1}}
\newcommand{\green}[1]{{\color{green}#1}}
\newcommand{\ans}[1]{{\color{orange}\textsf{Ans}: #1}}


\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand{\prob}[1]{\mathbb{P}\left(#1\right)}
\newcommand{\mean}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\var}[1]{\mathbb{V}\text{ar}\left(#1\right)}
\newcommand{\cov}[1]{\mathbb{C}\text{ov}\left(#1\right)}
\newcommand{\sign}[1]{\text{sign}\left(#1\right)}
\newcommand{\inner}[2]{\left\langle #1,#2\right\rangle}
\newcommand{\norm}[1]{\left\lVert #1\right\rVert}
\newcommand{\corr}[1]{\text{corr}\left(#1\right)}
\newcommand{\Xv}{\mathbf{X}}
\newcommand{\Yv}{\mathbf{Y}}
\newcommand{\Hj}{H_{-j}}
\newcommand{\tr}[1]{\text{tr}\left[#1\right]}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}




%===========================================================
\begin{document}
	\begin{center}
		\huge{\MBlue{\textbf{Homework 1}}}		
		\vskip20pt
		\large{
			\textbf{name:} Paolo Borello\\
            \textbf{email:} borello@umich.edu}
	\end{center}

    \vskip20pt
    \noindent
    \textbf{\large \MBlue{Exercise 1}}
    \vskip10pt
    \noindent
	\begin{exercise}[Solution]
        
    \end{exercise}

    \newpage
    \textbf{\large \MBlue{Exercise 2}}
    \vskip10pt
    \noindent
	\begin{exercise}[Solution]
        \begin{enumerate}[(a)]
            \item We have the multivariate linear regression model given by
                    \begin{align*}
                        \Yv = \Xv B + E
                    \end{align*}
                    with $\Yv$ an $N\times m$ matrix, $\Xv$ an $N\times p$ matrix, $B$ a $p\times m$ matrix and $E$ a $N\times m$ matrix where each row $\epsilon_i^\top\overset{\text{i.i.d.}}{\sim}\mathcal{N}_m\left(0,\Sigma\right)$.\\
                    Now we have that
                    \begin{align*}
                        Y_i = B^\top X_i + \epsilon_i \sim \mathcal{N}_m\left(B^\top X_i, \Sigma\right)
                    \end{align*}
                    therefore our log-likelihood is given by
                    \begin{align*}
                        \ell\left(B,\Sigma\right) &= c + \frac{N}{2}\log\abs{\Sigma^{-1}} - \frac{1}{2}\tr{\sum_{i=1}^{N}\left(Y_i-B^\top X_i\right)^\top \Sigma^{-1}\left(Y_i-B^\top X_i\right)} = \\
                        &= c + \frac{N}{2}\log\abs{\Sigma^{-1}} - \frac{1}{2}\tr{\Sigma^{-1}\sum_{i=1}^{N}\left(Y_i-B^\top X_i\right)\left(Y_i-B^\top X_i\right)^\top} = \\
                        &= c + \frac{N}{2}\log\abs{\Sigma^{-1}} - \frac{1}{2}\tr{\Sigma^{-1} \left(\Yv-\Xv B\right)\left(\Yv-\Xv B\right)^\top}
                    \end{align*}
                    Now notice that our log-likelihood objective is strictly concave therefore setting the gradients with respect to our parameters yields the unique maximizer. Moreover, setting the gradient with respect to $\Sigma$ to 0 is equivalent to setting the gradient with respect to $\Sigma^{-1}$ to 0.\\
                    Therefore taking the gradients with respect to $B$ and $\Sigma^{-1}$ yields
                    \begin{align*}
                        \nabla_B \ell\left(B,\Sigma\right) &= -\frac{1}{2}\nabla_B \tr{\Sigma^{-1} \left(\Yv-\Xv B\right)\left(\Yv-\Xv B\right)^\top} = \\
                        &= -\frac{1}{2}\left[\nabla_B\left(\Yv-\Xv B\right)\right] \left(\Yv-\Xv B\right)\left(\Sigma^{-1}+\Sigma^{-1}\right) = \\
                        &= \Xv^\top\left(\Yv-\Xv B\right)\Sigma^{-1} \overset{!}{=} 0 \implies \\
                        \hat{B} &= \left(\Xv^\top\Xv\right)^{-1}\Xv^\top\Yv\\
                        \nabla_{\Sigma^{-1}} \ell\left(B,\Sigma\right) &= \frac{N}{2} \Sigma ^\top -\frac{1}{2} \left(\Yv-\Xv B\right)^\top\left(\Yv-\Xv B\right) = \\
                        &=\frac{N}{2} \Sigma -\frac{1}{2} \left(\Yv-\Xv B\right)^\top\left(\Yv-\Xv B\right) = 0\implies\\
                        \hat{\Sigma} &= \frac{1}{N}\left(\Yv-\Xv \hat{B}\right)^\top\left(\Yv-\Xv \hat{B}\right) = \\
                        &= \frac{1}{N}\left(\Yv-\Xv \left(\Xv^\top\Xv\right)^{-1}\Xv^\top\Yv\right)^\top\left(\Yv-\Xv \left(\Xv^\top\Xv\right)^{-1}\Xv^\top\Yv\right) = \\
                        &= \frac{1}{N}\Yv^\top \left(I_N - \Xv \left(\Xv^\top\Xv\right)^{-1}\Xv^\top\right)^\top\left(I_N - \Xv \left(\Xv^\top\Xv\right)^{-1}\Xv^\top\right)\Yv = \\
                        &= \frac{1}{N}\Yv^\top \left(I_N - \Xv \left(\Xv^\top\Xv\right)^{-1}\Xv^\top\right)\Yv
                    \end{align*}
            \item Now we define the $\text{RSS}$ as 
                    \begin{align*}
                        \text{RSS}(B) = \sum_{i=1}^{N} \left(Y_i^\top - X_i^\top B\right)\left(Y_i^\top - X_i^\top B\right)^\top
                    \end{align*}
                    Then taking the gradient with respect to $B$ we have
                    \begin{align*}
                        \nabla_B\text{RSS}(B) &= \sum_{i=1}^{N}-2\left[\nabla_B\left(Y_i^\top - X_i^\top B\right)\right]\left(Y_i^\top - X_i^\top B\right) = \\
                        &= \sum_{i=1}^{N}-2 X_i \left(Y_i^\top - X_i^\top B\right) = \\
                        &= -2\sum_{i=1}^{N}X_i Y_i^\top + 2 \left(\sum_{i=1}^{N} X_i X_i^\top\right) B = \\
                        &= -2 \Xv^\top\Yv + 2 \Xv^\top\Xv B = 0 \implies\\
                        \hat{B} &= \left(\Xv^\top\Xv\right)^{-1}\Xv^\top\Yv
                    \end{align*}
                    thus yielding the same estimator as the MLE from part (a).
        \end{enumerate}
    \end{exercise}

    \newpage
    \textbf{\large \MBlue{Exercise 3}}
    \vskip10pt
    \noindent
	\begin{exercise}[Solution]

    \end{exercise}

    \newpage
    \textbf{\large \MBlue{Exercise 4}}
    \vskip10pt
    \noindent
	\begin{exercise}[Solution]

    \end{exercise}

\end{document}